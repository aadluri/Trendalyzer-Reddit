{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "335e027f",
   "metadata": {},
   "source": [
    "## Data Scraping From Reddit for Trend Analysis Model Building\n",
    "\n",
    "This notebook is focused on scraping data from Reddit to build a comprehensive dataset for trend analysis. The goal is to collect posts, comments, and metadata from various subreddits that are relevant to trending topics. This data will be used to train and evaluate machine learning models for trend prediction and creating dashboards. There will be a minimal version of this in a .py file, use this file for visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import sqlite3\n",
    "import time\n",
    "from src.config import REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, USER_AGENT, SUBREDDITS, POST_LIMIT, DB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1fc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_db():\n",
    "    \"\"\"Initialize database with schema.sql.\"\"\"\n",
    "    with open(\"schema.sql\", \"r\") as f:\n",
    "        schema = f.read()\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.cursor()\n",
    "    cur.executescript(schema)\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_reddit():\n",
    "    \"\"\"Connect to Reddit API using PRAW.\"\"\"\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=REDDIT_CLIENT_ID,\n",
    "        client_secret=REDDIT_CLIENT_SECRET,\n",
    "        user_agent=USER_AGENT\n",
    "    )\n",
    "    return reddit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_store(subreddit_name):\n",
    "    \"\"\"Scrape data from Reddit and store it in the database. Only top posts and comments.\"\"\"\n",
    "    reddit = connect_reddit()\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for post in subreddit.top(limit=POST_LIMIT):\n",
    "        cur.execute(\n",
    "            \"INSERT OR IGNORE INTO posts (id, title, score, created_utc, num_comments) VALUES (?, ?, ?, ?, ?)\",\n",
    "            (post.id, post.title, post.score, post.created_utc, post.num_comments)\n",
    "        )\n",
    "        post.comments.replace_more(limit=0)\n",
    "        for comment in post.comments.list():\n",
    "            cur.execute(\n",
    "                \"INSERT OR IGNORE INTO comments (id, post_id, body, score, created_utc) VALUES (?, ?, ?, ?, ?)\",\n",
    "                (comment.id, \n",
    "                post.id, \n",
    "                comment.body, \n",
    "                comment.score, \n",
    "                comment.created_utc)\n",
    "            )\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    init_db()\n",
    "    for sub in SUBREDDITS:\n",
    "        print(f\"Scraping {sub}...\")\n",
    "        scrape_comments(sub)\n",
    "    print(\"Scraping complete, data stored in data folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
